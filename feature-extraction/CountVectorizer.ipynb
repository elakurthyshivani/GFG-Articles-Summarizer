{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e861f4-dbe9-4a7b-8a90-88ea827af470",
   "metadata": {},
   "source": [
    "# **Count Vectorizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0fddf4-9972-4035-90b7-b69b6b96ad80",
   "metadata": {},
   "source": [
    "## **Run required Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5204b348-05ac-4697-a865-9df3bb876af8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.28.41)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.41 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.31.41)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (0.6.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.41->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.41->boto3) (1.26.14)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.41->boto3) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%run ../utilities/CassandraUtility.ipynb\n",
    "%run ../utilities/S3Utility.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794329a4-b402-4a5a-8824-340283db21de",
   "metadata": {},
   "source": [
    "## **Read database from Keyspaces using PySpark**\n",
    "\n",
    "**1.** Download the required jar files (`spark-cassandra-connector_2.12-3.3.0.jar, spark-cassandra-connector-assembly_2.12-3.3.0.jar`).\n",
    "\n",
    "**2.** Download your `cassandra_truststore.jks` file.\n",
    "\n",
    "**3.** Create `application.conf` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e242bf11-34f1-4661-9a33-e7bf425ad3d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24bfca03-c6ff-43ee-90e1-acda7dfdffd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/02 16:35:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark=createSparkSessionWithCassandraConf(\"CountVectorizer\")\n",
    "# Spark version\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d162612-c3ee-47f6-8457-9c3f22ba2e5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/02 16:35:57 WARN CassandraConnectionFactory: Ignoring all programmatic configuration, only using configuration from application.conf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|   ID| PreprocessedContent|\n",
      "+-----+--------------------+\n",
      "| 8772|consider follow p...|\n",
      "|11346|amcat amcat aspir...|\n",
      "|23825|online code round...|\n",
      "|23790|samsung r institu...|\n",
      "|13740|man command linux...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "articles=createDataFrameFromTable(spark, \"GFGArticles\", \"BasicPreprocessedGFGArticles\")\n",
    "articles.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c7eaaa-df15-4353-afcb-d8c5f7bf6650",
   "metadata": {},
   "source": [
    "## **Convert the preprocessed content into tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eedb2fc3-0382-46b6-a4bb-b8748331a996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aba86b0-ef92-4965-bde3-87c80072f74d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|   ID| PreprocessedContent|              Tokens|\n",
      "+-----+--------------------+--------------------+\n",
      "| 8772|consider follow p...|[consider, follow...|\n",
      "|11346|amcat amcat aspir...|[amcat, amcat, as...|\n",
      "|23825|online code round...|[online, code, ro...|\n",
      "|23790|samsung r institu...|[samsung, r, inst...|\n",
      "|13740|man command linux...|[man, command, li...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer=Tokenizer(inputCol=\"PreprocessedContent\", outputCol=\"Tokens\")\n",
    "articles=tokenizer.transform(articles).toDF(\"ID\", \"PreprocessedContent\", \"Tokens\")\n",
    "articles.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af24435-734b-486c-b5e2-04a768450a66",
   "metadata": {},
   "source": [
    "## **CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b48d3f38-0363-43cb-a3ee-d8ac86ba1543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc5aa11-44d2-45cf-a4f2-90f889a5bf89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|   ID| PreprocessedContent|              Tokens|         CountVector|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "| 8772|consider follow p...|[consider, follow...|(50000,[8,13,20,2...|\n",
      "|11346|amcat amcat aspir...|[amcat, amcat, as...|(50000,[9,14,16,1...|\n",
      "|23825|online code round...|[online, code, ro...|(50000,[0,1,2,4,6...|\n",
      "|23790|samsung r institu...|[samsung, r, inst...|(50000,[0,1,2,4,6...|\n",
      "|13740|man command linux...|[man, command, li...|(50000,[0,1,2,4,5...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countVec=CountVectorizer(inputCol=\"Tokens\", outputCol=\"Counts\", vocabSize=50000)\n",
    "countVecModel=countVec.fit(articles)\n",
    "articles=countVecModel.transform(articles).toDF(\"ID\", \"PreprocessedContent\", \"Tokens\", \"CountVector\")\n",
    "articles.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f314cbe-5441-4618-8d6a-adb7dae4d51b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = false)\n",
      " |-- PreprocessedContent: string (nullable = true)\n",
      " |-- Tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- CountVector: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look into the schema\n",
    "articles.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c86083c-3d35-4b2d-ade2-12e5d9e44994",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('ID', IntegerType(), False), StructField('PreprocessedContent', StringType(), True), StructField('Tokens', ArrayType(StringType(), True), True), StructField('CountVector', VectorUDT(), True)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look into the schema\n",
    "articles.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40322b4-40e5-44fe-a81d-249d444c12c5",
   "metadata": {},
   "source": [
    "## **Transform the `CountVector` into separate columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ba31544-8fc9-406f-8f73-bfadc9cb7901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, udf\n",
    "from pyspark.sql.types import ArrayType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73e50bb8-3d9f-44ea-90ae-b1b30e142ced",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   ID| PreprocessedContent|              Tokens|         CountVector|     FeaturesIndices|      FeaturesValues|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|17036|b b c c answer ex...|[b, b, c, c, answ...|(50000,[3,4,6,7,8...|[3, 4, 6, 7, 8, 9...|[13, 1, 1, 5, 1, ...|\n",
      "|16625|give two array ta...|[give, two, array...|(50000,[0,1,2,3,4...|[0, 1, 2, 3, 4, 5...|[6, 5, 3, 7, 5, 4...|\n",
      "|  188|c strcat function...|[c, strcat, funct...|(50000,[10,13,14,...|[10, 13, 14, 15, ...|[2, 1, 2, 1, 3, 1...|\n",
      "|  564|world programming...|[world, programmi...|(50000,[0,1,2,3,4...|[0, 1, 2, 3, 4, 6...|[3, 4, 3, 7, 2, 9...|\n",
      "|11971|article know appr...|[article, know, a...|(50000,[0,1,2,4,8...|[0, 1, 2, 4, 8, 1...|[3, 4, 1, 1, 12, ...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# articles=articles.withColumn(\"FeaturesCount\", udf(lambda countVector : countVector.size, \n",
    "#                                          IntegerType())(col(\"CountVector\")))\n",
    "articles=articles.withColumn(\"FeaturesIndices\", udf(lambda countVector : countVector.indices.tolist(), \n",
    "                                           ArrayType(IntegerType()))(col(\"CountVector\")))\n",
    "articles=articles.withColumn(\"FeaturesValues\", udf(lambda countVector : countVector.values.astype(np.int32).tolist(), \n",
    "                                           ArrayType(IntegerType()))(col(\"CountVector\")))\n",
    "articles.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76493276-326a-4cb0-97fd-d1f1ac5a4d9f",
   "metadata": {},
   "source": [
    "## **Write to a new table in Keyspaces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8334d64-c0da-48dd-acdb-a2326796fcbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6f354fa-ba8c-487d-b412-2448166314f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:================================>                         (5 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [33792, 34815] saved to GFGArticles.CountVectorGFGArticles.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "saveDataFrameToTable(articles[[\"ID\", \"FeaturesIndices\", \"FeaturesValues\"]], \"GFGArticles\", \"CountVectorGFGArticles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6d41c4-2e4d-4892-9e58-e07deb10b685",
   "metadata": {},
   "source": [
    "## **Save the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5b12f89-aaa2-4ee9-a9b4-0118383a6c00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the model. A folder with the given name will be created.\n",
    "countVecModel.write().overwrite().save(\"CountVectorizer\")\n",
    "# Save the model to S3.\n",
    "saveModelToS3(\"CountVectorizer\", \"shivmlstorage\", \"eshivani/GFG-Articles-Summarizer-Models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
