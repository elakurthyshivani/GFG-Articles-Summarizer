{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e861f4-dbe9-4a7b-8a90-88ea827af470",
   "metadata": {},
   "source": [
    "# **TFIDF Vectorizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928d06db-2ec7-4111-9fae-1b085b803874",
   "metadata": {},
   "source": [
    "## **Run required Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "703db3a2-831a-4758-ae11-bfbe0448644b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.28.41)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.41 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.31.41)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (0.6.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.41->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.41->boto3) (1.26.14)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.41->boto3) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%run ../utilities/CassandraUtility.ipynb\n",
    "%run ../utilities/S3Utility.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794329a4-b402-4a5a-8824-340283db21de",
   "metadata": {},
   "source": [
    "## **Read database from Keyspaces using PySpark**\n",
    "\n",
    "**1.** Download the required jar files (`spark-cassandra-connector_2.12-3.3.0.jar, spark-cassandra-connector-assembly_2.12-3.3.0.jar`).\n",
    "\n",
    "**2.** Download your `cassandra_truststore.jks` file.\n",
    "\n",
    "**3.** Create `application.conf` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e242bf11-34f1-4661-9a33-e7bf425ad3d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c6c840f-1b28-466d-869e-f8086fe9fb10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/02 16:36:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/02 16:36:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/10/02 16:36:53 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark=createSparkSessionWithCassandraConf(\"TFIDFVectorizer\")\n",
    "# Spark version\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c94a47af-b504-49f5-a1a2-7ee57d0454a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/02 16:36:56 WARN CassandraConnectionFactory: Ignoring all programmatic configuration, only using configuration from application.conf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|   ID| PreprocessedContent|\n",
      "+-----+--------------------+\n",
      "|29550|functional progra...|\n",
      "|28635|give integer n ta...|\n",
      "|24783|give integer n ar...|\n",
      "|23435|give integer n k ...|\n",
      "| 4382|problem use switc...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "articles=createDataFrameFromTable(spark, \"GFGArticles\", \"BasicPreprocessedGFGArticles\")\n",
    "articles.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c7eaaa-df15-4353-afcb-d8c5f7bf6650",
   "metadata": {},
   "source": [
    "## **Convert the preprocessed content into tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eedb2fc3-0382-46b6-a4bb-b8748331a996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aba86b0-ef92-4965-bde3-87c80072f74d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|   ID| PreprocessedContent|              Tokens|\n",
      "+-----+--------------------+--------------------+\n",
      "| 8772|consider follow p...|[consider, follow...|\n",
      "|11346|amcat amcat aspir...|[amcat, amcat, as...|\n",
      "|23825|online code round...|[online, code, ro...|\n",
      "|23790|samsung r institu...|[samsung, r, inst...|\n",
      "|13740|man command linux...|[man, command, li...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer=Tokenizer(inputCol=\"PreprocessedContent\", outputCol=\"Tokens\")\n",
    "articles=tokenizer.transform(articles).toDF(\"ID\", \"PreprocessedContent\", \"Tokens\")\n",
    "articles.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af24435-734b-486c-b5e2-04a768450a66",
   "metadata": {},
   "source": [
    "## **HashingTF and IDF Vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b48d3f38-0363-43cb-a3ee-d8ac86ba1543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc5aa11-44d2-45cf-a4f2-90f889a5bf89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|   ID| PreprocessedContent|              Tokens|            TFVector|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|17036|b b c c answer ex...|[b, b, c, c, answ...|(50000,[922,2189,...|\n",
      "|16625|give two array ta...|[give, two, array...|(50000,[223,1487,...|\n",
      "|  188|c strcat function...|[c, strcat, funct...|(50000,[453,573,1...|\n",
      "|  564|world programming...|[world, programmi...|(50000,[7,440,922...|\n",
      "|11971|article know appr...|[article, know, a...|(50000,[3095,5133...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfVec=HashingTF(inputCol=\"Tokens\", outputCol=\"TFVector\", numFeatures=50000)\n",
    "tfArticles=tfVec.transform(articles)\n",
    "tfArticles.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d3b375f-df32-4d34-9d20-c90dac5e48d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|   ID| PreprocessedContent|              Tokens|            TFVector|         TFIDFVector|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "| 2218|matplotlib highly...|[matplotlib, high...|(50000,[564,1119,...|(50000,[564,1119,...|\n",
      "|20243|give string str l...|[give, string, st...|(50000,[790,1659,...|(50000,[790,1659,...|\n",
      "|17714|c variable always...|[c, variable, alw...|(50000,[223,573,9...|(50000,[223,573,9...|\n",
      "|19583|want share interv...|[want, share, int...|(50000,[573,1264,...|(50000,[573,1264,...|\n",
      "|16870|string basic cove...|[string, basic, c...|(50000,[453,564,5...|(50000,[453,564,5...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idfVec=IDF(inputCol=\"TFVector\", outputCol=\"TFIDFVector\")\n",
    "idfVecModel=idfVec.fit(tfArticles)\n",
    "articles=idfVecModel.transform(tfArticles).toDF(\"ID\", \"PreprocessedContent\", \"Tokens\", \"TFVector\", \"TFIDFVector\")\n",
    "articles.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f314cbe-5441-4618-8d6a-adb7dae4d51b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = false)\n",
      " |-- PreprocessedContent: string (nullable = true)\n",
      " |-- Tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- TFVector: vector (nullable = true)\n",
      " |-- TFIDFVector: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look into the schema\n",
    "articles.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c86083c-3d35-4b2d-ade2-12e5d9e44994",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('ID', IntegerType(), False), StructField('PreprocessedContent', StringType(), True), StructField('Tokens', ArrayType(StringType(), True), True), StructField('TFVector', VectorUDT(), True), StructField('TFIDFVector', VectorUDT(), True)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look into the schema\n",
    "articles.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40322b4-40e5-44fe-a81d-249d444c12c5",
   "metadata": {},
   "source": [
    "## **Transform the `TFIDFVector` into separate columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ba31544-8fc9-406f-8f73-bfadc9cb7901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73e50bb8-3d9f-44ea-90ae-b1b30e142ced",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   ID| PreprocessedContent|              Tokens|            TFVector|         TFIDFVector|     FeaturesIndices|      FeaturesValues|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|29550|functional progra...|[functional, prog...|(50000,[573,585,1...|(50000,[573,585,1...|[573, 585, 1069, ...|[1, 3, 7, 2, 14, ...|\n",
      "|28635|give integer n ta...|[give, integer, n...|(50000,[86,585,92...|(50000,[86,585,92...|[86, 585, 922, 28...|[4, 1, 5, 2, 2, 1...|\n",
      "|24783|give integer n ar...|[give, integer, n...|(50000,[163,330,5...|(50000,[163,330,5...|[163, 330, 592, 1...|[6, 3, 3, 19, 2, ...|\n",
      "|23435|give integer n k ...|[give, integer, n...|(50000,[585,743,1...|(50000,[585,743,1...|[585, 743, 1364, ...|[4, 3, 5, 3, 9, 1...|\n",
      "| 4382|problem use switc...|[problem, use, sw...|(50000,[190,668,4...|(50000,[190,668,4...|[190, 668, 4043, ...|[11, 3, 1, 3, 0, ...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# articles=articles.withColumn(\"FeaturesCount\", udf(lambda tfidfVector : tfidfVector.size, \n",
    "#                                          IntegerType())(col(\"TFIDFVector\")))\n",
    "articles=articles.withColumn(\"FeaturesIndices\", udf(lambda tfidfVector : tfidfVector.indices.tolist(), \n",
    "                                           ArrayType(IntegerType()))(col(\"TFIDFVector\")))\n",
    "articles=articles.withColumn(\"FeaturesValues\", udf(lambda tfidfVector : tfidfVector.values.astype(np.int32).tolist(), \n",
    "                                           ArrayType(IntegerType()))(col(\"TFIDFVector\")))\n",
    "articles.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76493276-326a-4cb0-97fd-d1f1ac5a4d9f",
   "metadata": {},
   "source": [
    "## **Write to a new table in Keyspaces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8334d64-c0da-48dd-acdb-a2326796fcbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ff7aef5-c284-4125-9bb4-19d6002ce31a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [33792, 34815] saved to GFGArticles.TFIDFVectorGFGArticles.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "saveDataFrameToTable(articles[[\"ID\", \"FeaturesIndices\", \"FeaturesValues\"]], \"GFGArticles\", \"TFIDFVectorGFGArticles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad83d09-3071-4a25-9727-327053d13197",
   "metadata": {},
   "source": [
    "## **Save the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7779ef7-5d84-4c9a-b90b-cd167de23e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the model. A folder with the given name will be created.\n",
    "idfVecModel.write().overwrite().save(\"TFIDFVectorizer\")\n",
    "# Save the model to S3.\n",
    "saveModelToS3(\"TFIDFVectorizer\", \"shivmlstorage\", \"eshivani/GFG-Articles-Summarizer-Models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
