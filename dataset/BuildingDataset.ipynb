{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Install packages if not yet installed**"
      ],
      "metadata": {
        "id": "NSQ-wgPYo5-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "!{sys.executable} -m pip install bs4 # BeautifulSoup\n",
        "!{sys.executable} -m pip install opendatasets # OpenDatasets\n",
        "!{sys.executable} -m pip install pyspark # PySpark"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting bs4\n  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.11.2)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.4.1)\nBuilding wheels for collected packages: bs4\n  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1256 sha256=00cd76fa2dbb1d77fccdb4ecb76869dffec0d9cb3abeb3e13688a285838e2557\n  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\nSuccessfully built bs4\nInstalling collected packages: bs4\nSuccessfully installed bs4-0.0.1\nCollecting opendatasets\n  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.1)\nRequirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.16)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.6)\nRequirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2023.7.22)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\nRequirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.1)\nRequirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.4)\nRequirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.0.0)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\nRequirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.4)\nInstalling collected packages: opendatasets\nSuccessfully installed opendatasets-0.1.22\nCollecting pyspark\n  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285388 sha256=56b5d75bbd5a3fe66fd573885193e1a8ab74aecadd02e9298a043b1f815a5255\n  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.4.1\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wlAyi5eowBN",
        "outputId": "7de9caa1-05e8-43da-fefd-cf00f70159ba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Reading the dataset**\n",
        "\n",
        "**1.** Create a file `kaggle.json` and save your Kaggle username and API key. This will be used to download the dataset from Kaggle.\n",
        "\n",
        "**2.** The URL of the dataset is [https://www.kaggle.com/datasets/ashishjangra27/geeksforgeeks-articles](https://www.kaggle.com/datasets/ashishjangra27/geeksforgeeks-articles \"GeeksForGeeks Articles Dataset\"). Using `opendatasets` package, download the dataset. Step 1 is required in order for this to automatically take in your username and API key.\n",
        "\n",
        "**3.** Create a Spark Session to start working with PySpark.\n",
        "\n",
        "**4.** Read the downloaded dataset."
      ],
      "metadata": {
        "id": "0u_mKLnrrQDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import opendatasets as od\n",
        "from pyspark.sql import SparkSession"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "id": "_HY149OvspXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating kaggle.json file.\n",
        "with open(\"kaggle.json\", \"w\") as kaggleFile:\n",
        "    kaggleFile.write(json.dumps({\"username\":\"shivanielakurthy\", \"key\":\"da7b4ae4bd1b770cb8b74d3990fc7f43\"}))"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "id": "BwxV64rvrPQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the dataset.\n",
        "od.download(\"https://www.kaggle.com/datasets/ashishjangra27/geeksforgeeks-articles\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Downloading geeksforgeeks-articles.zip to ./geeksforgeeks-articles\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "100%|██████████| 1.31M/1.31M [00:00<00:00, 2.46MB/s]"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYGBnqsLpf96",
        "outputId": "b9722636-93ec-4f7a-f3c3-df1b4976a274"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark Session.\n",
        "spark=SparkSession.builder.config('spark.app.name', 'geeks_for_geeks_articles').getOrCreate()"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "id": "paVbMldtwpD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the dataset.\n",
        "articles=spark.read.option('header', True)\\\n",
        "          .option('inferSchema', True)\\\n",
        "          .csv(r\"geeksforgeeks-articles/articles.csv\")\n",
        "articles.show(5, truncate=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------------------------------------+----------------+------------+---------------------------------------------------------------------------+--------+\n|title                                       |author_id       |last_updated|link                                                                       |category|\n+--------------------------------------------+----------------+------------+---------------------------------------------------------------------------+--------+\n|5 Best Practices For Writing SQL Joins      |priyankab14     |21 Feb, 2022|https://www.geeksforgeeks.org/5-best-practices-for-writing-sql-joins/      |easy    |\n|Foundation CSS Dropdown Menu                |ishankhandelwals|20 Feb, 2022|https://www.geeksforgeeks.org/foundation-css-dropdown-menu/                |easy    |\n|Top 20 Excel Shortcuts That You Need To Know|priyankab14     |17 Feb, 2022|https://www.geeksforgeeks.org/top-20-excel-shortcuts-that-you-need-to-know/|easy    |\n|Servlet – Fetching Result                   |nishatiwari1719 |17 Feb, 2022|https://www.geeksforgeeks.org/servlet-fetching-result/                     |easy    |\n|Suffix Sum Array                            |rohit768        |21 Feb, 2022|https://www.geeksforgeeks.org/suffix-sum-array/                            |easy    |\n+--------------------------------------------+----------------+------------+---------------------------------------------------------------------------+--------+\nonly showing top 5 rows\n\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJZmKjR3pFJU",
        "outputId": "3e3d8b1d-ecf6-4aa7-d7fc-93ea4900a640"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dropping rows with null values**"
      ],
      "metadata": {
        "id": "7XBDpD3v0wvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "articles=articles.dropna()"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "id": "uFmwL_SBykL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating a pandas-on-Spark Dataframe**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.pandas as ps\n",
        "articles=ps.DataFrame(articles)\n",
        "articles.loc[1:5]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Scrap text from the URL to get article content**"
      ],
      "metadata": {
        "id": "Zd4cQrZkvA5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from pyspark.sql.functions import lit, col, udf\n",
        "import requests\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "id": "zsaLA04W0_Hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add new column to save the scrapped text from the URLs.\n",
        "articles[\"text\"]=\"\"\n",
        "articles.loc[1:5]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------------------------------------+----------------+------------+---------------------------------------------------------------------------+--------+----+\n|title                                       |author_id       |last_updated|link                                                                       |category|text|\n+--------------------------------------------+----------------+------------+---------------------------------------------------------------------------+--------+----+\n|5 Best Practices For Writing SQL Joins      |priyankab14     |21 Feb, 2022|https://www.geeksforgeeks.org/5-best-practices-for-writing-sql-joins/      |easy    |    |\n|Foundation CSS Dropdown Menu                |ishankhandelwals|20 Feb, 2022|https://www.geeksforgeeks.org/foundation-css-dropdown-menu/                |easy    |    |\n|Top 20 Excel Shortcuts That You Need To Know|priyankab14     |17 Feb, 2022|https://www.geeksforgeeks.org/top-20-excel-shortcuts-that-you-need-to-know/|easy    |    |\n|Servlet – Fetching Result                   |nishatiwari1719 |17 Feb, 2022|https://www.geeksforgeeks.org/servlet-fetching-result/                     |easy    |    |\n|Suffix Sum Array                            |rohit768        |21 Feb, 2022|https://www.geeksforgeeks.org/suffix-sum-array/                            |easy    |    |\n+--------------------------------------------+----------------+------------+---------------------------------------------------------------------------+--------+----+\nonly showing top 5 rows\n\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7FqxSgf1FJ_",
        "outputId": "1e7283a5-c609-49cc-e3d3-be63966802f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to save the errors occurred while scrapping text.\n",
        "scrapTextErrors={}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set timeout.\n",
        "TIMEOUT_SECS=60"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to scrap text.\n",
        "def scrapText(i, link):\n",
        "    try:\n",
        "        page=requests.get(link).text\n",
        "        parser=BeautifulSoup(page, \"html.parser\")\n",
        "\n",
        "        # Get the inner HTML of <div class=\"text\"></div> tag. This consists of the main content.\n",
        "        # Instead of recursively finding this tag with the above class name, I'm going iteratively to avoid max recursion errors.\n",
        "        parser=parser.find(\"html\", recursive=False)\n",
        "        parser=parser.find(\"body\", recursive=False)\n",
        "        parser=parser.find(\"div\", id=\"main\", recursive=False)\n",
        "        parser=parser.find(\"div\", id=\"home-page\", recursive=False)\n",
        "        parser=parser.find(\"div\", class_=\"article-page_flex\", recursive=False)\n",
        "        parser=parser.find(\"div\", class_=\"leftBar\", recursive=False)\n",
        "        parser=parser.find(\"div\", class_=\"article--viewer\", recursive=False)\n",
        "        parser=parser.find(\"div\", class_=\"article--viewer_content\", recursive=False)\n",
        "        parser=parser.find(\"div\", class_=\"a-wrapper\", recursive=False)\n",
        "        parser=parser.find(\"article\", recursive=False)\n",
        "        \n",
        "        text=[\"\"]\n",
        "        for tag in parser.find(\"div\", class_=\"text\", recursive=False).contents:\n",
        "            # Ignore all the <div> tags inside <div class=\"text\"></div> as they do not have any\n",
        "            # main content.\n",
        "            if tag.name!=\"div\":\n",
        "                text.append(\" \".join(tag.stripped_strings))\n",
        "        # Return the main content.\n",
        "        return i, \"\\n\".join(text).strip(\"\\n\")\n",
        "    \n",
        "    except Exception as err:\n",
        "        # print(f\"ScrapText error ({i}, {link}) : {err}\")\n",
        "        scrapTextErrors[i]={\"link\": link, \"error\": err}\n",
        "    return i, \"\""
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "id": "uc8ghB8KtOAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the above function for all the links using multithreading.\n",
        "%%time\n",
        "future_to_url={}\n",
        "futureResultErrors=[]\n",
        "with ThreadPoolExecutor(max_workers=1000) as executor:\n",
        "    for i in range(1, articles.shape[0]):\n",
        "        future_to_url[executor.submit(scrapText, i, articles.loc[i, \"link\"])]=i\n",
        "        \n",
        "    for future in as_completed(future_to_url):\n",
        "        try:\n",
        "            i, text=future.result(timeout=TIMEOUT_SECS)\n",
        "            articles.loc[i, \"text\"]=text\n",
        "        except Exception as err:\n",
        "            # print(f\"Future result error ({i}) : {err}\")\n",
        "            futureResultErrors.append(err)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View the value of the text column for any row.\n",
        "articles.loc[0, \"text\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of articles with empty values in the \"text\" column.\n",
        "articles.filter(articles[\"text\"]==\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Save errors to a file**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the futureResultErrors to the scrapTextErrors and save to a file.\n",
        "scrapTextErrors[\"futureResult\"]=futureResultErrors\n",
        "with open(\"ScrapTextErrors.json\", \"w\") as errorsFile:\n",
        "    errorsFile.write(json.dumps(scrapTextErrors))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Write to .parquet file**"
      ],
      "metadata": {
        "id": "LDuIClC1Ko3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write to Azure Blob Storage?"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "id": "Q8o1Zc-TKmkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stop the Spark session**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop the spark session.\n",
        "spark.stop()"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "id": "A43kxzemw4jE"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSztwXH+1bB2Q16wguXXUm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}